{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_Transpose.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeonggunlee/CUDATeaching/blob/master/02_cuda_lab/01_Transpose.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UT_qN20klON",
        "colab_type": "text"
      },
      "source": [
        "## Matrix Transpose Optimization\n",
        "\n",
        "![Matrix Transpose](https://2.bp.blogspot.com/-8LzbJv0zB3A/WAzRQbxP5eI/AAAAAAAAHYU/MEqPV8JxtLMCSGSQ-0UKZSYlUN3jALZaQCLcB/s1600/Java%2BProgram%2Bto%2BTranspose%2Ba%2BMatrix%2B.png)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJLKazMijoQF",
        "colab_type": "code",
        "outputId": "53fb8254-5dd7-43a9-91b7-fd8310a17951",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%%writefile copy.cu\n",
        "\n",
        "/* Copyright (c) 1993-2015, NVIDIA CORPORATION. All rights reserved.\n",
        " *\n",
        " * Redistribution and use in source and binary forms, with or without\n",
        " * modification, are permitted provided that the following conditions\n",
        " * are met:\n",
        " *  * Redistributions of source code must retain the above copyright\n",
        " *    notice, this list of conditions and the following disclaimer.\n",
        " *  * Redistributions in binary form must reproduce the above copyright\n",
        " *    notice, this list of conditions and the following disclaimer in the\n",
        " *    documentation and/or other materials provided with the distribution.\n",
        " *  * Neither the name of NVIDIA CORPORATION nor the names of its\n",
        " *    contributors may be used to endorse or promote products derived\n",
        " *    from this software without specific prior written permission.\n",
        " *\n",
        " * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY\n",
        " * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
        " * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n",
        " * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n",
        " * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n",
        " * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n",
        " * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n",
        " * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\n",
        " * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n",
        " * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
        " * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
        " */\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <assert.h>\n",
        "\n",
        "// Convenience function for checking CUDA runtime API results\n",
        "// can be wrapped around any runtime API call. No-op in release builds.\n",
        "inline\n",
        "cudaError_t checkCuda(cudaError_t result)\n",
        "{\n",
        "#if defined(DEBUG) || defined(_DEBUG)\n",
        "  if (result != cudaSuccess) {\n",
        "    fprintf(stderr, \"CUDA Runtime Error: %s\\n\", cudaGetErrorString(result));\n",
        "    assert(result == cudaSuccess);\n",
        "  }\n",
        "#endif\n",
        "  return result;\n",
        "}\n",
        "\n",
        "const int TILE_DIM = 32;\n",
        "const int BLOCK_ROWS = 8;\n",
        "const int NUM_REPS = 100;\n",
        "\n",
        "// Check errors and print GB/s\n",
        "void postprocess(const float *ref, const float *res, int n, float ms)\n",
        "{\n",
        "  bool passed = true;\n",
        "  for (int i = 0; i < n; i++)\n",
        "    if (res[i] != ref[i]) {\n",
        "      printf(\"%d %f %f\\n\", i, res[i], ref[i]);\n",
        "      printf(\"%25s\\n\", \"*** FAILED ***\");\n",
        "      passed = false;\n",
        "      break;\n",
        "    }\n",
        "  if (passed)\n",
        "    printf(\"%20.2f\\n\", 2 * n * sizeof(float) * 1e-6 * NUM_REPS / ms );\n",
        "}\n",
        "\n",
        "// simple copy kernel\n",
        "// Used as reference case representing best effective bandwidth.\n",
        "__global__ void copy(float *odata, const float *idata)\n",
        "{\n",
        "  int x = blockIdx.x * TILE_DIM + threadIdx.x;\n",
        "  int y = blockIdx.y * TILE_DIM + threadIdx.y;\n",
        "  int width = gridDim.x * TILE_DIM;\n",
        "\n",
        "  for (int j = 0; j < TILE_DIM; j+= BLOCK_ROWS)\n",
        "    odata[(y+j)*width + x] = idata[(y+j)*width + x];\n",
        "}\n",
        "\n",
        "// copy kernel using shared memory\n",
        "// Also used as reference case, demonstrating effect of using shared memory.\n",
        "__global__ void copySharedMem(float *odata, const float *idata)\n",
        "{\n",
        "  __shared__ float tile[TILE_DIM * TILE_DIM];\n",
        "  \n",
        "  int x = blockIdx.x * TILE_DIM + threadIdx.x;\n",
        "  int y = blockIdx.y * TILE_DIM + threadIdx.y;\n",
        "  int width = gridDim.x * TILE_DIM;\n",
        "\n",
        "  for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS)\n",
        "     tile[(threadIdx.y+j)*TILE_DIM + threadIdx.x] = idata[(y+j)*width + x];\n",
        "\n",
        "  __syncthreads();\n",
        "\n",
        "  for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS)\n",
        "     odata[(y+j)*width + x] = tile[(threadIdx.y+j)*TILE_DIM + threadIdx.x];          \n",
        "}\n",
        "\n",
        "\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "  const int nx = 1024;\n",
        "  const int ny = 1024;\n",
        "  const int mem_size = nx*ny*sizeof(float);\n",
        "\n",
        "  dim3 dimGrid(nx/TILE_DIM, ny/TILE_DIM, 1);\n",
        "  dim3 dimBlock(TILE_DIM, BLOCK_ROWS, 1);\n",
        "\n",
        "  int devId = 0;\n",
        "  if (argc > 1) devId = atoi(argv[1]);\n",
        "\n",
        "  cudaDeviceProp prop;\n",
        "  checkCuda( cudaGetDeviceProperties(&prop, devId));\n",
        "  printf(\"\\nDevice : %s\\n\", prop.name);\n",
        "  printf(\"Matrix size: %d %d, Block size: %d %d, Tile size: %d %d\\n\", \n",
        "         nx, ny, TILE_DIM, BLOCK_ROWS, TILE_DIM, TILE_DIM);\n",
        "  printf(\"dimGrid: %d %d %d. dimBlock: %d %d %d\\n\",\n",
        "         dimGrid.x, dimGrid.y, dimGrid.z, dimBlock.x, dimBlock.y, dimBlock.z);\n",
        "  \n",
        "  checkCuda( cudaSetDevice(devId) );\n",
        "\n",
        "  float *h_idata = (float*)malloc(mem_size);\n",
        "  float *h_cdata = (float*)malloc(mem_size);\n",
        "  float *h_tdata = (float*)malloc(mem_size);\n",
        "  float *gold    = (float*)malloc(mem_size);\n",
        "  \n",
        "  float *d_idata, *d_cdata, *d_tdata;\n",
        "  checkCuda( cudaMalloc(&d_idata, mem_size) );\n",
        "  checkCuda( cudaMalloc(&d_cdata, mem_size) );\n",
        "  checkCuda( cudaMalloc(&d_tdata, mem_size) );\n",
        "\n",
        "  // check parameters and calculate execution configuration\n",
        "  if (nx % TILE_DIM || ny % TILE_DIM) {\n",
        "    printf(\"nx and ny must be a multiple of TILE_DIM\\n\");\n",
        "    goto error_exit;\n",
        "  }\n",
        "\n",
        "  if (TILE_DIM % BLOCK_ROWS) {\n",
        "    printf(\"TILE_DIM must be a multiple of BLOCK_ROWS\\n\");\n",
        "    goto error_exit;\n",
        "  }\n",
        "    \n",
        "  // host\n",
        "  for (int j = 0; j < ny; j++)\n",
        "    for (int i = 0; i < nx; i++)\n",
        "      h_idata[j*nx + i] = j*nx + i;\n",
        "\n",
        "  // correct result for error checking\n",
        "  for (int j = 0; j < ny; j++)\n",
        "    for (int i = 0; i < nx; i++)\n",
        "      gold[j*nx + i] = h_idata[i*nx + j];\n",
        "  \n",
        "  // device\n",
        "  checkCuda( cudaMemcpy(d_idata, h_idata, mem_size, cudaMemcpyHostToDevice) );\n",
        "  \n",
        "  // events for timing\n",
        "  cudaEvent_t startEvent, stopEvent;\n",
        "  checkCuda( cudaEventCreate(&startEvent) );\n",
        "  checkCuda( cudaEventCreate(&stopEvent) );\n",
        "  float ms;\n",
        "\n",
        "  // ------------\n",
        "  // time kernels\n",
        "  // ------------\n",
        "  printf(\"%25s%25s\\n\", \"Routine\", \"Bandwidth (GB/s)\");\n",
        "  \n",
        "  // ----\n",
        "  // copy \n",
        "  // ----\n",
        "  printf(\"%25s\", \"copy\");\n",
        "  checkCuda( cudaMemset(d_cdata, 0, mem_size) );\n",
        "  // warm up\n",
        "  copy<<<dimGrid, dimBlock>>>(d_cdata, d_idata);\n",
        "  checkCuda( cudaEventRecord(startEvent, 0) );\n",
        "  for (int i = 0; i < NUM_REPS; i++)\n",
        "     copy<<<dimGrid, dimBlock>>>(d_cdata, d_idata);\n",
        "  checkCuda( cudaEventRecord(stopEvent, 0) );\n",
        "  checkCuda( cudaEventSynchronize(stopEvent) );\n",
        "  checkCuda( cudaEventElapsedTime(&ms, startEvent, stopEvent) );\n",
        "  checkCuda( cudaMemcpy(h_cdata, d_cdata, mem_size, cudaMemcpyDeviceToHost) );\n",
        "  postprocess(h_idata, h_cdata, nx*ny, ms);\n",
        "\n",
        "  // -------------\n",
        "  // copySharedMem \n",
        "  // -------------\n",
        "  printf(\"%25s\", \"shared memory copy\");\n",
        "  checkCuda( cudaMemset(d_cdata, 0, mem_size) );\n",
        "  // warm up\n",
        "  copySharedMem<<<dimGrid, dimBlock>>>(d_cdata, d_idata);\n",
        "  checkCuda( cudaEventRecord(startEvent, 0) );\n",
        "  for (int i = 0; i < NUM_REPS; i++)\n",
        "     copySharedMem<<<dimGrid, dimBlock>>>(d_cdata, d_idata);\n",
        "  checkCuda( cudaEventRecord(stopEvent, 0) );\n",
        "  checkCuda( cudaEventSynchronize(stopEvent) );\n",
        "  checkCuda( cudaEventElapsedTime(&ms, startEvent, stopEvent) );\n",
        "  checkCuda( cudaMemcpy(h_cdata, d_cdata, mem_size, cudaMemcpyDeviceToHost) );\n",
        "  postprocess(h_idata, h_cdata, nx * ny, ms);\n",
        "\n",
        "\n",
        "error_exit:\n",
        "  // cleanup\n",
        "  checkCuda( cudaEventDestroy(startEvent) );\n",
        "  checkCuda( cudaEventDestroy(stopEvent) );\n",
        "  checkCuda( cudaFree(d_tdata) );\n",
        "  checkCuda( cudaFree(d_cdata) );\n",
        "  checkCuda( cudaFree(d_idata) );\n",
        "  free(h_idata);\n",
        "  free(h_tdata);\n",
        "  free(h_cdata);\n",
        "  free(gold);\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing copy.cu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7E6xCy9kEM1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvcc -o copy copy.cu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7STxyl7kKRg",
        "colab_type": "code",
        "outputId": "f83b927c-d6c6-47e0-bc14-5955d2a7958c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "!./copy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Device : Tesla T4\n",
            "Matrix size: 1024 1024, Block size: 32 8, Tile size: 32 32\n",
            "dimGrid: 32 32 1. dimBlock: 32 8 1\n",
            "                  Routine         Bandwidth (GB/s)\n",
            "                     copy              212.45\n",
            "       shared memory copy              229.76\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BubEgUIk3YH",
        "colab_type": "code",
        "outputId": "c73bb687-d4c4-420e-a8b7-c6b693bc2e40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        }
      },
      "source": [
        "!nvprof ./copy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==467== NVPROF is profiling process 467, command: ./copy\n",
            "\n",
            "Device : Tesla T4\n",
            "Matrix size: 1024 1024, Block size: 32 8, Tile size: 32 32\n",
            "dimGrid: 32 32 1. dimBlock: 32 8 1\n",
            "                  Routine         Bandwidth (GB/s)\n",
            "                     copy              208.78\n",
            "       shared memory copy              224.01\n",
            "==467== Profiling application: ./copy\n",
            "==467== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   37.03%  3.9160ms       101  38.772us  38.334us  39.198us  copy(float*, float const *)\n",
            "                   34.08%  3.6045ms       101  35.688us  33.886us  36.798us  copySharedMem(float*, float const *)\n",
            "                   22.20%  2.3477ms         2  1.1739ms  563.74us  1.7840ms  [CUDA memcpy DtoH]\n",
            "                    6.67%  705.65us         1  705.65us  705.65us  705.65us  [CUDA memcpy HtoD]\n",
            "                    0.03%  2.7200us         2  1.3600us  1.2480us  1.4720us  [CUDA memset]\n",
            "      API calls:   94.26%  230.29ms         3  76.764ms  260.36us  229.74ms  cudaMalloc\n",
            "                    2.67%  6.5217ms         2  3.2608ms  3.0404ms  3.4813ms  cudaEventSynchronize\n",
            "                    2.03%  4.9516ms         3  1.6505ms  854.88us  3.2276ms  cudaMemcpy\n",
            "                    0.51%  1.2432ms       202  6.1540us  5.0740us  25.293us  cudaLaunchKernel\n",
            "                    0.24%  589.79us         3  196.60us  160.88us  215.67us  cudaFree\n",
            "                    0.08%  206.03us         1  206.03us  206.03us  206.03us  cuDeviceTotalMem\n",
            "                    0.07%  178.55us         1  178.55us  178.55us  178.55us  cudaGetDeviceProperties\n",
            "                    0.07%  172.82us        96  1.8000us     141ns  69.629us  cuDeviceGetAttribute\n",
            "                    0.04%  86.952us         2  43.476us  42.219us  44.733us  cudaMemset\n",
            "                    0.01%  26.614us         1  26.614us  26.614us  26.614us  cuDeviceGetName\n",
            "                    0.01%  13.857us         2  6.9280us  1.4880us  12.369us  cudaEventCreate\n",
            "                    0.01%  12.592us         4  3.1480us  1.8720us  4.4230us  cudaEventRecord\n",
            "                    0.00%  8.7270us         2  4.3630us  3.7980us  4.9290us  cudaEventElapsedTime\n",
            "                    0.00%  6.3380us         2  3.1690us     993ns  5.3450us  cudaEventDestroy\n",
            "                    0.00%  4.5980us         1  4.5980us  4.5980us  4.5980us  cudaSetDevice\n",
            "                    0.00%  3.5630us         1  3.5630us  3.5630us  3.5630us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.7570us         3     585ns     133ns  1.1560us  cuDeviceGetCount\n",
            "                    0.00%  1.3980us         2     699ns     222ns  1.1760us  cuDeviceGet\n",
            "                    0.00%     288ns         1     288ns     288ns     288ns  cuDeviceGetUuid\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5I_sbPLh_kn",
        "colab_type": "code",
        "outputId": "5aeb7bc8-9bb7-4613-f0b0-06168247ddb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%%writefile transpose.cu\n",
        "\n",
        "/* Copyright (c) 1993-2015, NVIDIA CORPORATION. All rights reserved.\n",
        " *\n",
        " * Redistribution and use in source and binary forms, with or without\n",
        " * modification, are permitted provided that the following conditions\n",
        " * are met:\n",
        " *  * Redistributions of source code must retain the above copyright\n",
        " *    notice, this list of conditions and the following disclaimer.\n",
        " *  * Redistributions in binary form must reproduce the above copyright\n",
        " *    notice, this list of conditions and the following disclaimer in the\n",
        " *    documentation and/or other materials provided with the distribution.\n",
        " *  * Neither the name of NVIDIA CORPORATION nor the names of its\n",
        " *    contributors may be used to endorse or promote products derived\n",
        " *    from this software without specific prior written permission.\n",
        " *\n",
        " * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY\n",
        " * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
        " * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n",
        " * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n",
        " * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n",
        " * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n",
        " * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n",
        " * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\n",
        " * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n",
        " * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
        " * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
        " */\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <assert.h>\n",
        "\n",
        "// Convenience function for checking CUDA runtime API results\n",
        "// can be wrapped around any runtime API call. No-op in release builds.\n",
        "inline\n",
        "cudaError_t checkCuda(cudaError_t result)\n",
        "{\n",
        "#if defined(DEBUG) || defined(_DEBUG)\n",
        "  if (result != cudaSuccess) {\n",
        "    fprintf(stderr, \"CUDA Runtime Error: %s\\n\", cudaGetErrorString(result));\n",
        "    assert(result == cudaSuccess);\n",
        "  }\n",
        "#endif\n",
        "  return result;\n",
        "}\n",
        "\n",
        "const int TILE_DIM = 32;\n",
        "const int BLOCK_ROWS = 8;\n",
        "const int NUM_REPS = 100;\n",
        "\n",
        "// Check errors and print GB/s\n",
        "void postprocess(const float *ref, const float *res, int n, float ms)\n",
        "{\n",
        "  bool passed = true;\n",
        "  for (int i = 0; i < n; i++)\n",
        "    if (res[i] != ref[i]) {\n",
        "      printf(\"%d %f %f\\n\", i, res[i], ref[i]);\n",
        "      printf(\"%25s\\n\", \"*** FAILED ***\");\n",
        "      passed = false;\n",
        "      break;\n",
        "    }\n",
        "  if (passed)\n",
        "    printf(\"%20.2f\\n\", 2 * n * sizeof(float) * 1e-6 * NUM_REPS / ms );\n",
        "}\n",
        "\n",
        "// naive transpose\n",
        "// Simplest transpose; doesn't use shared memory.\n",
        "// Global memory reads are coalesced but writes are not.\n",
        "__global__ void transposeNaive(float *odata, const float *idata)\n",
        "{\n",
        "  int x = blockIdx.x * TILE_DIM + threadIdx.x;\n",
        "  int y = blockIdx.y * TILE_DIM + threadIdx.y;\n",
        "  int width = gridDim.x * TILE_DIM;\n",
        "\n",
        "  for (int j = 0; j < TILE_DIM; j+= BLOCK_ROWS)\n",
        "    odata[x*width + (y+j)] = idata[(y+j)*width + x];\n",
        "}\n",
        "\n",
        "// coalesced transpose\n",
        "// Uses shared memory to achieve coalesing in both reads and writes\n",
        "// Tile width == #banks causes shared memory bank conflicts.\n",
        "__global__ void transposeCoalesced(float *odata, const float *idata)\n",
        "{\n",
        "  __shared__ float tile[TILE_DIM][TILE_DIM];\n",
        "    \n",
        "  int x = blockIdx.x * TILE_DIM + threadIdx.x;\n",
        "  int y = blockIdx.y * TILE_DIM + threadIdx.y;\n",
        "  int width = gridDim.x * TILE_DIM;\n",
        "\n",
        "  for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS)\n",
        "     tile[threadIdx.y+j][threadIdx.x] = idata[(y+j)*width + x];\n",
        "\n",
        "  __syncthreads();\n",
        "\n",
        "  x = blockIdx.y * TILE_DIM + threadIdx.x;  // transpose block offset\n",
        "  y = blockIdx.x * TILE_DIM + threadIdx.y;\n",
        "\n",
        "  for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS)\n",
        "     odata[(y+j)*width + x] = tile[threadIdx.x][threadIdx.y + j];\n",
        "}\n",
        "   \n",
        "\n",
        "// No bank-conflict transpose\n",
        "// Same as transposeCoalesced except the first tile dimension is padded \n",
        "// to avoid shared memory bank conflicts.\n",
        "__global__ void transposeNoBankConflicts(float *odata, const float *idata)\n",
        "{\n",
        "  __shared__ float tile[TILE_DIM][TILE_DIM+1];\n",
        "    \n",
        "  int x = blockIdx.x * TILE_DIM + threadIdx.x;\n",
        "  int y = blockIdx.y * TILE_DIM + threadIdx.y;\n",
        "  int width = gridDim.x * TILE_DIM;\n",
        "\n",
        "  for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS)\n",
        "     tile[threadIdx.y+j][threadIdx.x] = idata[(y+j)*width + x];\n",
        "\n",
        "  __syncthreads();\n",
        "\n",
        "  x = blockIdx.y * TILE_DIM + threadIdx.x;  // transpose block offset\n",
        "  y = blockIdx.x * TILE_DIM + threadIdx.y;\n",
        "\n",
        "  for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS)\n",
        "     odata[(y+j)*width + x] = tile[threadIdx.x][threadIdx.y + j];\n",
        "}\n",
        "\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "  const int nx = 1024;\n",
        "  const int ny = 1024;\n",
        "  const int mem_size = nx*ny*sizeof(float);\n",
        "\n",
        "  dim3 dimGrid(nx/TILE_DIM, ny/TILE_DIM, 1);\n",
        "  dim3 dimBlock(TILE_DIM, BLOCK_ROWS, 1);\n",
        "\n",
        "  int devId = 0;\n",
        "  if (argc > 1) devId = atoi(argv[1]);\n",
        "\n",
        "  cudaDeviceProp prop;\n",
        "  checkCuda( cudaGetDeviceProperties(&prop, devId));\n",
        "  printf(\"\\nDevice : %s\\n\", prop.name);\n",
        "  printf(\"Matrix size: %d %d, Block size: %d %d, Tile size: %d %d\\n\", \n",
        "         nx, ny, TILE_DIM, BLOCK_ROWS, TILE_DIM, TILE_DIM);\n",
        "  printf(\"dimGrid: %d %d %d. dimBlock: %d %d %d\\n\",\n",
        "         dimGrid.x, dimGrid.y, dimGrid.z, dimBlock.x, dimBlock.y, dimBlock.z);\n",
        "  \n",
        "  checkCuda( cudaSetDevice(devId) );\n",
        "\n",
        "  float *h_idata = (float*)malloc(mem_size);\n",
        "  float *h_cdata = (float*)malloc(mem_size);\n",
        "  float *h_tdata = (float*)malloc(mem_size);\n",
        "  float *gold    = (float*)malloc(mem_size);\n",
        "  \n",
        "  float *d_idata, *d_cdata, *d_tdata;\n",
        "  checkCuda( cudaMalloc(&d_idata, mem_size) );\n",
        "  checkCuda( cudaMalloc(&d_cdata, mem_size) );\n",
        "  checkCuda( cudaMalloc(&d_tdata, mem_size) );\n",
        "\n",
        "  // check parameters and calculate execution configuration\n",
        "  if (nx % TILE_DIM || ny % TILE_DIM) {\n",
        "    printf(\"nx and ny must be a multiple of TILE_DIM\\n\");\n",
        "    goto error_exit;\n",
        "  }\n",
        "\n",
        "  if (TILE_DIM % BLOCK_ROWS) {\n",
        "    printf(\"TILE_DIM must be a multiple of BLOCK_ROWS\\n\");\n",
        "    goto error_exit;\n",
        "  }\n",
        "    \n",
        "  // host\n",
        "  for (int j = 0; j < ny; j++)\n",
        "    for (int i = 0; i < nx; i++)\n",
        "      h_idata[j*nx + i] = j*nx + i;\n",
        "\n",
        "  // correct result for error checking\n",
        "  for (int j = 0; j < ny; j++)\n",
        "    for (int i = 0; i < nx; i++)\n",
        "      gold[j*nx + i] = h_idata[i*nx + j];\n",
        "  \n",
        "  // device\n",
        "  checkCuda( cudaMemcpy(d_idata, h_idata, mem_size, cudaMemcpyHostToDevice) );\n",
        "  \n",
        "  // events for timing\n",
        "  cudaEvent_t startEvent, stopEvent;\n",
        "  checkCuda( cudaEventCreate(&startEvent) );\n",
        "  checkCuda( cudaEventCreate(&stopEvent) );\n",
        "  float ms;\n",
        "\n",
        "  // ------------\n",
        "  // time kernels\n",
        "  // ------------\n",
        "  printf(\"%25s%25s\\n\", \"Routine\", \"Bandwidth (GB/s)\");\n",
        "\n",
        "  // --------------\n",
        "  // transposeNaive \n",
        "  // --------------\n",
        "  printf(\"%25s\", \"naive transpose\");\n",
        "  checkCuda( cudaMemset(d_tdata, 0, mem_size) );\n",
        "  // warmup\n",
        "  transposeNaive<<<dimGrid, dimBlock>>>(d_tdata, d_idata);\n",
        "  checkCuda( cudaEventRecord(startEvent, 0) );\n",
        "  for (int i = 0; i < NUM_REPS; i++)\n",
        "     transposeNaive<<<dimGrid, dimBlock>>>(d_tdata, d_idata);\n",
        "  checkCuda( cudaEventRecord(stopEvent, 0) );\n",
        "  checkCuda( cudaEventSynchronize(stopEvent) );\n",
        "  checkCuda( cudaEventElapsedTime(&ms, startEvent, stopEvent) );\n",
        "  checkCuda( cudaMemcpy(h_tdata, d_tdata, mem_size, cudaMemcpyDeviceToHost) );\n",
        "  postprocess(gold, h_tdata, nx * ny, ms);\n",
        "\n",
        "  // ------------------\n",
        "  // transposeCoalesced \n",
        "  // ------------------\n",
        "  printf(\"%25s\", \"coalesced transpose\");\n",
        "  checkCuda( cudaMemset(d_tdata, 0, mem_size) );\n",
        "  // warmup\n",
        "  transposeCoalesced<<<dimGrid, dimBlock>>>(d_tdata, d_idata);\n",
        "  checkCuda( cudaEventRecord(startEvent, 0) );\n",
        "  for (int i = 0; i < NUM_REPS; i++)\n",
        "     transposeCoalesced<<<dimGrid, dimBlock>>>(d_tdata, d_idata);\n",
        "  checkCuda( cudaEventRecord(stopEvent, 0) );\n",
        "  checkCuda( cudaEventSynchronize(stopEvent) );\n",
        "  checkCuda( cudaEventElapsedTime(&ms, startEvent, stopEvent) );\n",
        "  checkCuda( cudaMemcpy(h_tdata, d_tdata, mem_size, cudaMemcpyDeviceToHost) );\n",
        "  postprocess(gold, h_tdata, nx * ny, ms);\n",
        "\n",
        "  // ------------------------\n",
        "  // transposeNoBankConflicts\n",
        "  // ------------------------\n",
        "  printf(\"%25s\", \"conflict-free transpose\");\n",
        "  checkCuda( cudaMemset(d_tdata, 0, mem_size) );\n",
        "  // warmup\n",
        "  transposeNoBankConflicts<<<dimGrid, dimBlock>>>(d_tdata, d_idata);\n",
        "  checkCuda( cudaEventRecord(startEvent, 0) );\n",
        "  for (int i = 0; i < NUM_REPS; i++)\n",
        "     transposeNoBankConflicts<<<dimGrid, dimBlock>>>(d_tdata, d_idata);\n",
        "  checkCuda( cudaEventRecord(stopEvent, 0) );\n",
        "  checkCuda( cudaEventSynchronize(stopEvent) );\n",
        "  checkCuda( cudaEventElapsedTime(&ms, startEvent, stopEvent) );\n",
        "  checkCuda( cudaMemcpy(h_tdata, d_tdata, mem_size, cudaMemcpyDeviceToHost) );\n",
        "  postprocess(gold, h_tdata, nx * ny, ms);\n",
        "\n",
        "error_exit:\n",
        "  // cleanup\n",
        "  checkCuda( cudaEventDestroy(startEvent) );\n",
        "  checkCuda( cudaEventDestroy(stopEvent) );\n",
        "  checkCuda( cudaFree(d_tdata) );\n",
        "  checkCuda( cudaFree(d_cdata) );\n",
        "  checkCuda( cudaFree(d_idata) );\n",
        "  free(h_idata);\n",
        "  free(h_tdata);\n",
        "  free(h_cdata);\n",
        "  free(gold);\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting transpose.cu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4qZUeGJiyX5",
        "colab_type": "code",
        "outputId": "e055e10a-698d-4a12-d722-d4b1b0375afc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "copy  copy.cu  sample_data  transpose  transpose.cu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPqutafhi1Q4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvcc -o transpose transpose.cu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MX15rCYjKg3",
        "colab_type": "code",
        "outputId": "98484f08-3bc9-4fed-b358-79ddcca9ae46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "source": [
        "!./transpose"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Device : Tesla T4\n",
            "Matrix size: 1024 1024, Block size: 32 8, Tile size: 32 32\n",
            "dimGrid: 32 32 1. dimBlock: 32 8 1\n",
            "                  Routine         Bandwidth (GB/s)\n",
            "          naive transpose               44.46\n",
            "      coalesced transpose               74.86\n",
            "  conflict-free transpose              203.88\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vizv5p5ck9Pv",
        "colab_type": "code",
        "outputId": "029590d9-46b9-435c-951f-acfc187be2cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        }
      },
      "source": [
        "!nvprof ./transpose"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==484== NVPROF is profiling process 484, command: ./transpose\n",
            "\n",
            "Device : Tesla T4\n",
            "Matrix size: 1024 1024, Block size: 32 8, Tile size: 32 32\n",
            "dimGrid: 32 32 1. dimBlock: 32 8 1\n",
            "                  Routine         Bandwidth (GB/s)\n",
            "          naive transpose               44.41\n",
            "      coalesced transpose               74.84\n",
            "  conflict-free transpose              212.24\n",
            "==484== Profiling application: ./transpose\n",
            "==484== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   50.52%  18.932ms       101  187.45us  187.22us  190.58us  transposeNaive(float*, float const *)\n",
            "                   29.84%  11.181ms       101  110.70us  108.95us  112.60us  transposeCoalesced(float*, float const *)\n",
            "                   10.23%  3.8325ms       101  37.945us  32.318us  40.606us  transposeNoBankConflicts(float*, float const *)\n",
            "                    7.35%  2.7536ms         3  917.87us  515.07us  1.6767ms  [CUDA memcpy DtoH]\n",
            "                    2.05%  769.29us         1  769.29us  769.29us  769.29us  [CUDA memcpy HtoD]\n",
            "                    0.01%  3.7760us         3  1.2580us  1.2160us  1.3440us  [CUDA memset]\n",
            "      API calls:   85.21%  239.56ms         3  79.855ms  413.54us  238.73ms  cudaMalloc\n",
            "                   11.50%  32.346ms         3  10.782ms  3.3063ms  18.395ms  cudaEventSynchronize\n",
            "                    2.07%  5.8293ms         4  1.4573ms  805.91us  3.1713ms  cudaMemcpy\n",
            "                    0.70%  1.9669ms       303  6.4910us  5.3850us  25.505us  cudaLaunchKernel\n",
            "                    0.23%  645.18us         3  215.06us  199.02us  234.42us  cudaFree\n",
            "                    0.08%  214.14us         1  214.14us  214.14us  214.14us  cuDeviceTotalMem\n",
            "                    0.06%  166.10us        96  1.7300us     156ns  71.347us  cuDeviceGetAttribute\n",
            "                    0.06%  160.42us         1  160.42us  160.42us  160.42us  cudaGetDeviceProperties\n",
            "                    0.05%  147.22us         3  49.072us  38.171us  57.355us  cudaMemset\n",
            "                    0.01%  28.323us         1  28.323us  28.323us  28.323us  cuDeviceGetName\n",
            "                    0.01%  25.116us         1  25.116us  25.116us  25.116us  cuDeviceGetPCIBusId\n",
            "                    0.01%  19.809us         6  3.3010us  1.9370us  4.5630us  cudaEventRecord\n",
            "                    0.01%  14.233us         2  7.1160us  1.1030us  13.130us  cudaEventCreate\n",
            "                    0.00%  9.0310us         3  3.0100us  2.4740us  4.0780us  cudaEventElapsedTime\n",
            "                    0.00%  7.0850us         2  3.5420us  1.0020us  6.0830us  cudaEventDestroy\n",
            "                    0.00%  4.5840us         1  4.5840us  4.5840us  4.5840us  cudaSetDevice\n",
            "                    0.00%  2.5640us         3     854ns     223ns  1.7140us  cuDeviceGetCount\n",
            "                    0.00%  1.5130us         2     756ns     374ns  1.1390us  cuDeviceGet\n",
            "                    0.00%     279ns         1     279ns     279ns     279ns  cuDeviceGetUuid\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJ8YLJJTjM3G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}