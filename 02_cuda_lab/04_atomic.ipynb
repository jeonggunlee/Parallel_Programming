{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_atomic.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeonggunlee/CUDATeaching/blob/master/02_cuda_lab/04_atomic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5TzSbHmXJdZ",
        "colab_type": "text"
      },
      "source": [
        "## Testing Atomic Operation !\n",
        "\n",
        "Please check the youtube video : [Atomic Memory Operations - Intro to Parallel Programming](https://www.youtube.com/watch?v=r-WtkvzKcVA)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftRy1pmLWFR3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "331825c8-8ebc-4717-b11a-8d2c6b73f641"
      },
      "source": [
        "%%writefile atomic.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include \"gputimer.h\"\n",
        "#define NUM_THREADS 10000000\n",
        "#define ARRAY_SIZE  100\n",
        "#define BLOCK_WIDTH 1000\n",
        "\n",
        "void print_array(int *array, int size)\n",
        "{\n",
        "    printf(\"{ \");\n",
        "    for (int i = 0; i < size; i++)  { printf(\"%d \", array[i]); }\n",
        "    printf(\"}\\n\");\n",
        "}\n",
        "\n",
        "__global__ void increment_naive(int *g)\n",
        "{\n",
        "\t// which thread is this?\n",
        "\tint i = blockIdx.x * blockDim.x + threadIdx.x; \n",
        "\n",
        "  // each thread to increment consecutive elements, wrapping at ARRAY_SIZE\n",
        "\ti = i % ARRAY_SIZE;  \n",
        "\tg[i] = g[i] + 1;\n",
        "}\n",
        "\n",
        "__global__ void increment_atomic(int *g)\n",
        "{\n",
        "\t// which thread is this?\n",
        "\tint i = blockIdx.x * blockDim.x + threadIdx.x; \n",
        "\n",
        "  // each thread to increment consecutive elements, wrapping at ARRAY_SIZE\n",
        "\ti = i % ARRAY_SIZE;  \n",
        "\tatomicAdd(& g[i], 1);\n",
        "}\n",
        "\n",
        "int main(int argc,char **argv)\n",
        "{   \n",
        "    GpuTimer timer;\n",
        "    printf(\"%d total threads in %d blocks writing into %d array elements\\n\",\n",
        "           NUM_THREADS, NUM_THREADS / BLOCK_WIDTH, ARRAY_SIZE);\n",
        "\n",
        "    // declare and allocate host memory\n",
        "    int h_array[ARRAY_SIZE];\n",
        "    const int ARRAY_BYTES = ARRAY_SIZE * sizeof(int);\n",
        "\n",
        "    // declare, allocate, and zero out GPU memory\n",
        "    int * d_array;\n",
        "\n",
        "    cudaMalloc((void **) &d_array, ARRAY_BYTES);\n",
        "    cudaMemset((void *) d_array, 0, ARRAY_BYTES); \n",
        "\n",
        "    // launch the kernel - comment out one of these\n",
        "    timer.Start();\n",
        "\n",
        "    // Instructions: This program is needed for the next quiz\n",
        "    // uncomment increment_naive to measure speed and accuracy \n",
        "    // of non-atomic increments or uncomment increment_atomic to\n",
        "    // measure speed and accuracy of  atomic icrements\n",
        "    increment_naive<<<NUM_THREADS/BLOCK_WIDTH, BLOCK_WIDTH>>>(d_array);\n",
        "    //increment_atomic<<<NUM_THREADS/BLOCK_WIDTH, BLOCK_WIDTH>>>(d_array);\n",
        "    timer.Stop();   \n",
        "\n",
        "    // copy back the array of sums from GPU and print\n",
        "    cudaMemcpy(h_array, d_array, ARRAY_BYTES, cudaMemcpyDeviceToHost);\n",
        "    print_array(h_array, ARRAY_SIZE);\n",
        "    printf(\"Time elapsed = %g ms\\n\", timer.Elapsed());\n",
        "\n",
        "    // free GPU memory allocation and exit\n",
        "    cudaFree(d_array);\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting atomic.cu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQEYoHuUWmL9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "31bb7334-a000-49ff-8483-e0c81a11ab54"
      },
      "source": [
        "%%writefile gputimer.h\n",
        "\n",
        "#ifndef __GPU_TIMER_H__\n",
        "#define __GPU_TIMER_H__\n",
        "\n",
        "struct GpuTimer\n",
        "{\n",
        "      cudaEvent_t start;\n",
        "      cudaEvent_t stop;\n",
        " \n",
        "      GpuTimer()\n",
        "      {\n",
        "            cudaEventCreate(&start);\n",
        "            cudaEventCreate(&stop);\n",
        "      }\n",
        " \n",
        "      ~GpuTimer()\n",
        "      {\n",
        "            cudaEventDestroy(start);\n",
        "            cudaEventDestroy(stop);\n",
        "      }\n",
        " \n",
        "      void Start()\n",
        "      {\n",
        "            cudaEventRecord(start, 0);\n",
        "      }\n",
        " \n",
        "      void Stop()\n",
        "      {\n",
        "            cudaEventRecord(stop, 0);\n",
        "      }\n",
        " \n",
        "      float Elapsed()\n",
        "      {\n",
        "            float elapsed;\n",
        "            cudaEventSynchronize(stop);\n",
        "            cudaEventElapsedTime(&elapsed, start, stop);\n",
        "            return elapsed;\n",
        "      }\n",
        "};\n",
        "\n",
        "#endif  /* __GPU_TIMER_H__ */"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing gputimer.h\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kKVgJpnWJFV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvcc -o atomic atomic.cu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvuUZSHXXO0w",
        "colab_type": "text"
      },
      "source": [
        "## with atomic operation\n",
        "\n",
        "    // increment_naive<<<NUM_THREADS/BLOCK_WIDTH, BLOCK_WIDTH>>>(d_array);\n",
        "    \n",
        "    increment_atomic<<<NUM_THREADS/BLOCK_WIDTH, BLOCK_WIDTH>>>(d_array);"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3B7O1VY3WPl-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "7ca6dccc-f58f-4ba8-fd2a-84d363db81c9"
      },
      "source": [
        "!./atomic"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000000 total threads in 10000 blocks writing into 100 array elements\n",
            "{ 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 100000 }\n",
            "Time elapsed = 1.49888 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAA-RvGZXTbf",
        "colab_type": "text"
      },
      "source": [
        "## without atomic operation\n",
        "\n",
        "    increment_naive<<<NUM_THREADS/BLOCK_WIDTH, BLOCK_WIDTH>>>(d_array);\n",
        "    \n",
        "    //increment_atomic<<<NUM_THREADS/BLOCK_WIDTH, BLOCK_WIDTH>>>(d_array);"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5_LSzp7W54Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "bc968432-6866-4d65-9092-c28a980e6051"
      },
      "source": [
        "!./atomic"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000000 total threads in 10000 blocks writing into 100 array elements\n",
            "{ 251 251 251 251 251 251 251 251 251 251 251 251 251 251 251 251 251 251 251 251 251 251 251 251 251 251 251 251 251 251 251 251 251 251 251 251 251 251 251 251 251 251 251 251 251 251 251 251 251 251 251 251 251 251 251 251 251 251 251 251 251 251 251 251 250 250 250 250 250 250 250 250 250 250 250 250 250 250 250 250 250 250 250 250 250 250 250 250 250 250 250 250 250 250 250 250 251 251 251 251 }\n",
            "Time elapsed = 0.237568 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R578xGaqW5sH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}