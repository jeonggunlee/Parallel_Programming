{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "07_memoryType.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeonggunlee/CUDATeaching/blob/master/01_cuda_lab/07_memoryType.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jB9CAgYq9M9A",
        "colab_type": "text"
      },
      "source": [
        "## Memory Architecutre\n",
        "### GPU의 메모리 구조를 고려한 최적 Coding\n",
        "\n",
        "- Local Memory\n",
        "- Global Memory\n",
        "- Shared Memory\n",
        "\n",
        "### 참조\n",
        "\n",
        "- https://github.com/jeonggunlee/cs344"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zIW-1c583Rl",
        "colab_type": "code",
        "outputId": "b8fc62cb-d607-490b-bead-1e0001e1fc46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%%writefile memoryType.cu\n",
        "\n",
        "// Convenience function for checking CUDA runtime API results\n",
        "// can be wrapped around any runtime API call. No-op in release builds.\n",
        "inline\n",
        "cudaError_t checkCuda(cudaError_t result)\n",
        "{\n",
        "#if defined(DEBUG) || defined(_DEBUG)\n",
        "  if (result != cudaSuccess) {\n",
        "    fprintf(stderr, \"CUDA Runtime Error: %s\\n\", cudaGetErrorString(result));\n",
        "    assert(result == cudaSuccess);\n",
        "  }\n",
        "#endif\n",
        "  return result;\n",
        "}\n",
        "\n",
        "// CUDA에서 제공하는 서로 다른 타입의 메모리 공간 활용하기\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "/**********************\n",
        " * using local memory *\n",
        " **********************/\n",
        "\n",
        "// a __device__ or __global__ function runs on the GPU\n",
        "__global__ void use_local_memory_GPU(float in)\n",
        "{\n",
        "    float f;    // variable \"f\" is in local memory and private to each thread\n",
        "    f = in;     // parameter \"in\" is in local memory and private to each thread\n",
        "    // ... real code would presumably do other stuff here ... \n",
        "\n",
        "    // ADDED\n",
        "    int i, index = threadIdx.x;\n",
        "    float average, sum = 0.0f;\n",
        "\n",
        "    for (i=0; i<index; i++) { sum += i; }\n",
        "    average = sum / (index + 1.0f);\n",
        "\n",
        "    __syncthreads();    // ensure all the writes to shared memory have completed\n",
        "    \n",
        "}\n",
        "\n",
        "/**********************\n",
        " * using global memory *\n",
        " **********************/\n",
        "\n",
        "// a __global__ function runs on the GPU & can be called from host\n",
        "__global__ void use_global_memory_GPU(float *array)\n",
        "{\n",
        "    int i, index = threadIdx.x;\n",
        "    float average, sum = 0.0f;\n",
        "\n",
        "    // \"array\" is a pointer into global memory on the device\n",
        "    // array[threadIdx.x] = 2.0f * (float) threadIdx.x;\n",
        "\n",
        "    for (i=0; i<index; i++) { sum += array[i]; }\n",
        "    average = sum / (index + 1.0f);\n",
        "    \n",
        "    __syncthreads();    // ensure all the writes to shared memory have completed\n",
        "\n",
        "    if (array[index] > average) { array[index] = average; }     \n",
        "}\n",
        "\n",
        "/**********************\n",
        " * using shared memory *\n",
        " **********************/\n",
        "\n",
        "// (for clarity, hardcoding 128 threads/elements and omitting out-of-bounds checks)\n",
        "__global__ void use_shared_memory_GPU(float *array)\n",
        "{\n",
        "    // local variables, private to each thread\n",
        "    int i, index = threadIdx.x;\n",
        "    float average, sum = 0.0f;\n",
        "\n",
        "    // __shared__ variables are visible to all threads in the thread block\n",
        "    // and have the same lifetime as the thread block\n",
        "    __shared__ float sh_arr[128];\n",
        "\n",
        "    // copy data from \"array\" in global memory to sh_arr in shared memory.\n",
        "    // here, each thread is responsible for copying a single element.\n",
        "    sh_arr[index] = array[index];\n",
        "\n",
        "    __syncthreads();    // ensure all the writes to shared memory have completed\n",
        "\n",
        "    // now, sh_arr is fully populated. Let's find the average of all previous elements\n",
        "    for (i=0; i<index; i++) { sum += sh_arr[i]; }\n",
        "    average = sum / (index + 1.0f);\n",
        "\n",
        "    if (array[index] > average) { array[index] = average; } \n",
        "\n",
        "}\n",
        "\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "    int blockSize = 256;\n",
        "    int nBlock= 1024;\n",
        "    float ms;\n",
        "    cudaEvent_t startEvent, stopEvent;\n",
        "    \n",
        "    checkCuda( cudaEventCreate(&startEvent) );\n",
        "    checkCuda( cudaEventCreate(&stopEvent) );\n",
        "    \n",
        "    /*\n",
        "     * First, call a kernel that shows using local memory \n",
        "     */\n",
        "    checkCuda( cudaEventRecord(startEvent,0) );\n",
        "    use_local_memory_GPU<<<nBlock, blockSize>>>(2.0f);\n",
        "    checkCuda( cudaEventRecord(stopEvent,0) );\n",
        "    checkCuda( cudaEventSynchronize(stopEvent) );\n",
        "    checkCuda( cudaEventElapsedTime(&ms, startEvent, stopEvent) );\n",
        "    printf(\"Local : %f\\n\", ms);\n",
        "    \n",
        "    /*\n",
        "     * Next, call a kernel that shows using global memory\n",
        "     */\n",
        "    float h_arr[blockSize];   // convention: h_ variables live on host\n",
        "    float *d_arr;       // convention: d_ variables live on device (GPU global mem)\n",
        "\n",
        "    // allocate global memory on the device, place result in \"d_arr\"\n",
        "    cudaMalloc((void **) &d_arr, sizeof(float) * blockSize);\n",
        "    // now copy data from host memory \"h_arr\" to device memory \"d_arr\"\n",
        "    cudaMemcpy((void *)d_arr, (void *)h_arr, sizeof(float) * blockSize, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // launch the kernel (1 block of 128 threads)\n",
        "    checkCuda( cudaEventRecord(startEvent,0) );\n",
        "    use_global_memory_GPU<<<nBlock, blockSize>>>(d_arr);  // modifies the contents of array at d_arr\n",
        "    checkCuda( cudaEventRecord(stopEvent,0) );\n",
        "    checkCuda( cudaEventSynchronize(stopEvent) );\n",
        "    checkCuda( cudaEventElapsedTime(&ms, startEvent, stopEvent) );\n",
        "    printf(\"Global: %f\\n\", ms);\n",
        "\n",
        "    \n",
        "    // copy the modified array back to the host, overwriting contents of h_arr\n",
        "    cudaMemcpy((void *)h_arr, (void *)d_arr, sizeof(float) * blockSize, cudaMemcpyDeviceToHost);\n",
        "    // ... do other stuff ...\n",
        "\n",
        "    /*\n",
        "     * Next, call a kernel that shows using shared memory\n",
        "     */\n",
        "\n",
        "    // as before, pass in a pointer to data in global memory\n",
        "    checkCuda( cudaEventRecord(startEvent,0) );\n",
        "    use_shared_memory_GPU<<<nBlock, blockSize>>>(d_arr); \n",
        "    checkCuda( cudaEventRecord(stopEvent,0) );\n",
        "    checkCuda( cudaEventSynchronize(stopEvent) );\n",
        "    checkCuda( cudaEventElapsedTime(&ms, startEvent, stopEvent) );\n",
        "    printf(\"Shared: %f\\n\", ms);\n",
        "\n",
        "    \n",
        "    // copy the modified array back to the host\n",
        "    cudaMemcpy((void *)h_arr, (void *)d_arr, sizeof(float) * blockSize, cudaMemcpyHostToDevice);\n",
        "    // ... do other stuff ...\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting memoryType.cu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cuuy1wx9C6q",
        "colab_type": "code",
        "outputId": "503a2f08-992b-4de1-f71a-d0aa2b3bd74e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "!nvcc -o memoryType memoryType.cu"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "memoryType.cu(27): warning: variable \"f\" was set but never used\n",
            "\n",
            "memoryType.cu(33): warning: variable \"average\" was set but never used\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jY_CmtQ-9FIB",
        "colab_type": "code",
        "outputId": "9f4435ba-822e-47fc-e7bb-e4266b14d1f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "!./memoryType"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Local : 0.020672\n",
            "Global: 0.122144\n",
            "Shared: 0.122144\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTQ2WVPM9HVa",
        "colab_type": "code",
        "outputId": "9a30137c-d3a9-45a5-a437-d614d031f086",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        }
      },
      "source": [
        "!nvprof ./memoryType"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==683== NVPROF is profiling process 683, command: ./memoryType\n",
            "Local : 0.039360\n",
            "Global: 0.026976\n",
            "Shared: 0.026976\n",
            "==683== Profiling application: ./memoryType\n",
            "==683== Warning: 5 records have invalid timestamps due to insufficient device buffer space. You can configure the buffer space using the option --device-buffer-size.\n",
            "==683== Warning: 2 records have invalid timestamps due to insufficient semaphore pool size. You can configure the pool size using the option --profiling-semaphore-pool-size.\n",
            "==683== Profiling result:\n",
            "No kernels were profiled.\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            "      API calls:   85.54%  1.28212s         3  427.37ms  9.2140us  1.28210s  cudaEventSynchronize\n",
            "                   14.41%  215.93ms         2  107.96ms  1.1800us  215.93ms  cudaEventCreate\n",
            "                    0.02%  330.56us         1  330.56us  330.56us  330.56us  cudaMalloc\n",
            "                    0.01%  175.42us         1  175.42us  175.42us  175.42us  cuDeviceTotalMem\n",
            "                    0.01%  153.00us        96  1.5930us     131ns  62.566us  cuDeviceGetAttribute\n",
            "                    0.00%  60.460us         3  20.153us  11.842us  28.395us  cudaLaunchKernel\n",
            "                    0.00%  56.360us         3  18.786us  3.2680us  30.320us  cudaMemcpy\n",
            "                    0.00%  25.164us         1  25.164us  25.164us  25.164us  cuDeviceGetName\n",
            "                    0.00%  19.051us         6  3.1750us  2.2870us  5.1510us  cudaEventRecord\n",
            "                    0.00%  10.373us         3  3.4570us  2.6280us  5.0840us  cudaEventElapsedTime\n",
            "                    0.00%  2.8170us         1  2.8170us  2.8170us  2.8170us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.0260us         3     675ns     148ns  1.3160us  cuDeviceGetCount\n",
            "                    0.00%  1.4640us         2     732ns     282ns  1.1820us  cuDeviceGet\n",
            "                    0.00%     243ns         1     243ns     243ns     243ns  cuDeviceGetUuid\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDm9OcUwR7sk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}