{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_helloCUDA.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeonggunlee/CUDATeaching/blob/master/01_cuda_lab/04_helloCUDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJsm8qxkByZK",
        "colab_type": "text"
      },
      "source": [
        "## CUDA Basic Programming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mFiaU4S9FJ2",
        "colab_type": "text"
      },
      "source": [
        "##GPU 코딩의 기본 단계 익히기\n",
        "\n",
        "- CPU 메모리 설정\n",
        "- CPU 메모리 데이터 설정\n",
        "- GPU 메모리 설정 : cudaMalloc(...)\n",
        "- CPU --> GPU 데이터 전송: cudaMemcpy(to, from, sizeofdata, cudaMemcpyHostToDevice)\n",
        "- GPU 함수 (Kernel) 수행\n",
        "- GPU --> CPU 연산 결과 데이터 전송: : cudaMemcpy(to, from, sizeofdata, cudaMemcpyDeviceToHost);\n",
        "- 연산 결과를 CPU에서 사용\n",
        "\n",
        "*  *  *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_UOYChfF_jf",
        "colab_type": "text"
      },
      "source": [
        "##n 값을 변경시켜가며 nvprof을 통해서 수행시간 및 데이터 전달 시간을 살펴보세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8oWrLbL9mut",
        "colab_type": "code",
        "outputId": "fa6dcd52-7496-4b54-fe04-9b38c6f3d915",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%%writefile cudabasic.cu\n",
        "\n",
        "#include <iostream>\n",
        "#include <cuda.h>\n",
        "using namespace std;\n",
        "\n",
        "int *host_A, *host_C1, *host_C2;       // host data\n",
        "int *device_A, *device_C;   // results\n",
        "\n",
        "__global__ void vecAddOne(int *A, int *C, int N)\n",
        "{\n",
        "   int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    \n",
        "   if( i < N )\n",
        "      C[i] = A[i] + 1; \n",
        "}\n",
        "\n",
        "void vecAddOne_h(int *A1, int *C1, int N)\n",
        "{\n",
        "   for(int i=0;i<N;i++)\n",
        "      C1[i] = A1[i] + 1;\n",
        "}\n",
        "\n",
        "int main(int argc,char **argv)\n",
        "{\n",
        "   int n=1024*1024;\n",
        "   int nBytes = n*sizeof(int);\n",
        "   int block_size = 32, block_no = n / block_size; \n",
        "\n",
        "   // ===============================================================\n",
        "   // CPU 메모리 설정 \n",
        "   //\n",
        "   host_A = (int *)malloc(nBytes);\n",
        "   host_C1 = (int *)malloc(nBytes);    \n",
        "   host_C2 = (int *)malloc(nBytes);    \n",
        "\n",
        "   // ===============================================================    \n",
        "   printf(\"Allocating device memory on host..\\n\");\n",
        "   cudaMalloc((void **)&device_A, n*sizeof(int));\n",
        "   cudaMalloc((void **)&device_C, n*sizeof(int));\n",
        "   // ===============================================================    \n",
        "   printf(\"Copying to device..\\n\");\n",
        "   cudaMemcpy(device_A, host_A, n*sizeof(int),cudaMemcpyHostToDevice);\n",
        "   // ===============================================================\n",
        "   printf(\"Doing GPU Vector + 1 \\n\");\n",
        "   vecAddOne<<<block_no,block_size>>>(device_A, device_C, n);   \n",
        "   cudaDeviceSynchronize();\n",
        "   // ===============================================================\n",
        "   printf(\"Doing a CPU Vector add\\n\");    \n",
        "   vecAddOne_h(host_A, host_C1, n);\n",
        "   \n",
        "   cudaMemcpy(host_C2, device_C, n*sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "   // 결과 비교\n",
        "   printf(\"결과 비교\\n\");\n",
        "   for(int i=0; i<n;i++)\n",
        "   {\n",
        "       if(host_C1[i] != host_C2[i])\n",
        "       {\n",
        "           printf(\"Something Wrong ! \\n\");\n",
        "           break;\n",
        "       }\n",
        "   }\n",
        "   cudaFree(device_A);\n",
        "   cudaFree(device_C);\n",
        "   free(host_A);\n",
        "   free(host_C1);\n",
        "   free(host_C2);\n",
        "   return 0;\n",
        "}  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting cudabasic.cu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulc4tfsuBHsO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvcc -o cudabasic cudabasic.cu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mp12avb1BcWe",
        "colab_type": "code",
        "outputId": "652543c6-8125-43a9-b920-1ecdefbf89f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "!./cudabasic"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Allocating device memory on host..\n",
            "Copying to device..\n",
            "Doing GPU Vector + 1 \n",
            "Doing a CPU Vector add\n",
            "결과 비교\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwC2L5UUGMtH",
        "colab_type": "text"
      },
      "source": [
        "*  *  *\n",
        "*  *  *\n",
        "## nvprof:\n",
        "\n",
        "\n",
        "참조:\n",
        "\n",
        "- https://devblogs.nvidia.com/cuda-pro-tip-nvprof-your-handy-universal-gpu-profiler/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vMCjO8ZFMMv",
        "colab_type": "code",
        "outputId": "17bb0333-053a-4180-faf0-eba1dd43ec59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        }
      },
      "source": [
        "!nvprof ./cudabasic"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Allocating device memory on host..\n",
            "==835== NVPROF is profiling process 835, command: ./cudabasic\n",
            "Copying to device..\n",
            "Doing GPU Vector + 1 \n",
            "Doing a CPU Vector add\n",
            "결과 비교\n",
            "==835== Profiling application: ./cudabasic\n",
            "==835== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   55.53%  1.6226ms         1  1.6226ms  1.6226ms  1.6226ms  [CUDA memcpy DtoH]\n",
            "                   40.72%  1.1897ms         1  1.1897ms  1.1897ms  1.1897ms  [CUDA memcpy HtoD]\n",
            "                    3.75%  109.72us         1  109.72us  109.72us  109.72us  vecAddOne(int*, int*, int)\n",
            "      API calls:   97.66%  228.51ms         2  114.26ms  294.69us  228.22ms  cudaMalloc\n",
            "                    1.94%  4.5426ms         2  2.2713ms  1.5016ms  3.0410ms  cudaMemcpy\n",
            "                    0.16%  363.22us         2  181.61us  145.53us  217.69us  cudaFree\n",
            "                    0.09%  203.17us         1  203.17us  203.17us  203.17us  cuDeviceTotalMem\n",
            "                    0.07%  153.12us         1  153.12us  153.12us  153.12us  cudaDeviceSynchronize\n",
            "                    0.06%  149.82us        96  1.5600us     130ns  64.457us  cuDeviceGetAttribute\n",
            "                    0.02%  39.996us         1  39.996us  39.996us  39.996us  cudaLaunchKernel\n",
            "                    0.01%  26.604us         1  26.604us  26.604us  26.604us  cuDeviceGetName\n",
            "                    0.00%  2.9230us         1  2.9230us  2.9230us  2.9230us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.3790us         3     793ns     166ns  1.7490us  cuDeviceGetCount\n",
            "                    0.00%  1.2660us         2     633ns     285ns     981ns  cuDeviceGet\n",
            "                    0.00%     246ns         1     246ns     246ns     246ns  cuDeviceGetUuid\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPdlxjtknBLn",
        "colab_type": "text"
      },
      "source": [
        "*  *  *\n",
        "*  *  *\n",
        "\n",
        "**Kernel**: function that executes on device (GPU) and can be called from host (CPU)\n",
        "\n",
        "- Functions must be declared with a qualifier\n",
        "   - \\_\\_global\\_\\_: GPU kernel function launched by CPU, must return void\n",
        "   - \\_\\_device\\_\\_: can be called from GPU functions\n",
        "   - \\_\\_host\\_\\_: can be called from CPU functions (default)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0U4KP73rm2WG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "de20780c-cdb1-4182-f170-cb367e9b583c"
      },
      "source": [
        "%%writefile cudaQual.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "__device__ void hiDeviceFunction(void)\n",
        "{ printf(\"Hello! This is in hiDeviceFunction. \\n\");}\n",
        "\n",
        "__global__ void helloCUDA(void)\n",
        "{\n",
        "  printf(\"Hello thread %d\\n\", threadIdx.x);\n",
        "  hiDeviceFunction();\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "  helloCUDA<<<1, 1>>>();\n",
        "  return 0;\n",
        "}\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing cudaQual.cu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1p15nY_m1cA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvcc -o cudaQual cudaQual.cu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMyDUdZzncd-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!./cudaQual"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5fTQlsenhGu",
        "colab_type": "text"
      },
      "source": [
        "위의 코드의 결과 값이 우리가 예상하는 값인가요 ?\n",
        "\n",
        "우리가 예상한 값이 나오지 않았다면 그 이유는 무엇일까요 ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hR4DTteiDXKg",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "---\n",
        "\n",
        "## Thread와 Block 이해하기\n",
        "\n",
        "- blockIdx.x / blockIdx.y\n",
        "- blockDim.x / blockDim.y\n",
        "- threadIdx.x / threadIdx.y\n",
        "- threadDim.x / threadDim.y\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6wkCid3W5Dn",
        "colab_type": "code",
        "outputId": "e742fec3-8f6d-4dd6-d39a-5ed042a06d36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%%writefile helloCUDA1.cu\n",
        "\n",
        "// 여러개의 블럭을 만들기! --------------> 10개의 블럭\n",
        "// 각 블럭에는 하나의 쓰레드만 포함 -----> 블럭당 1개의 쓰레드\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__ void helloCUDA(void)\n",
        "{\n",
        "  printf(\"Hello thread %d in block %d\\n\", threadIdx.x, blockIdx.x);\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "    \n",
        "  helloCUDA<<<10, 1>>>();\n",
        "    \n",
        "  cudaDeviceSynchronize();  // printf 함수가 완료될 때 까지 대기\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing helloCUDA1.cu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuFZjuhtXAls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvcc -o helloCUDA1 helloCUDA1.cu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Mi0z-KzXzP_",
        "colab_type": "code",
        "outputId": "2650d0b7-b0c4-46c6-b9b6-c6972e9c0c59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "!./helloCUDA1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello thread 0 in block 4\n",
            "Hello thread 0 in block 9\n",
            "Hello thread 0 in block 8\n",
            "Hello thread 0 in block 3\n",
            "Hello thread 0 in block 0\n",
            "Hello thread 0 in block 1\n",
            "Hello thread 0 in block 5\n",
            "Hello thread 0 in block 2\n",
            "Hello thread 0 in block 7\n",
            "Hello thread 0 in block 6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qg6res3bX58t",
        "colab_type": "code",
        "outputId": "0e5d714b-cf39-439b-dd10-40acdcb6eb8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%%writefile helloCUDA2.cu\n",
        "\n",
        "// 하나의 블럭을 만들기! --------------> 1개의 블럭\n",
        "// 각 블럭에는 10개의 쓰레드 포함 -----> 블럭당 10개의 쓰레드\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__ void helloCUDA(void)\n",
        "{\n",
        "  printf(\"Hello thread %d in block %d\\n\", threadIdx.x, blockIdx.x);\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "    \n",
        "  helloCUDA<<<1, 10>>>();\n",
        "    \n",
        "  cudaDeviceSynchronize();  // printf 함수가 완료될 때 까지 대기\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing helloCUDA2.cu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqbZh4Q8YLvf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvcc -o helloCUDA2 helloCUDA2.cu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBj7Q3DGYOKd",
        "colab_type": "code",
        "outputId": "236b126b-6905-44fd-befc-3596a2046a87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "!./helloCUDA2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello thread 0 in block 0\n",
            "Hello thread 1 in block 0\n",
            "Hello thread 2 in block 0\n",
            "Hello thread 3 in block 0\n",
            "Hello thread 4 in block 0\n",
            "Hello thread 5 in block 0\n",
            "Hello thread 6 in block 0\n",
            "Hello thread 7 in block 0\n",
            "Hello thread 8 in block 0\n",
            "Hello thread 9 in block 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGRYhJOWBs8p",
        "colab_type": "code",
        "outputId": "af5d5022-23d5-40a7-f554-48d833f9ea10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%%writefile helloCUDA3.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__ void helloCUDA(void)\n",
        "{\n",
        "  printf(\"Hello thread %d in block %d\\n\", threadIdx.x, blockIdx.x);\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "  int n = 12;\n",
        "  int blockDim = 4;            // Block내의 Thread의 수\n",
        "  int gridDim = n / blockDim;  // Grid에서 Block의 수\n",
        "  \n",
        "  // 따라서, 전체 생성 thread의 수는 blockDim * threadDim  \n",
        "    \n",
        "  helloCUDA<<<gridDim, blockDim>>>();\n",
        "    \n",
        "  cudaDeviceSynchronize();\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing helloCUDA3.cu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftSMqLF6B6gB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvcc -o helloCUDA3 helloCUDA3.cu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwaUXUeECDi5",
        "colab_type": "code",
        "outputId": "c4e623b6-d105-4a27-9756-0581989338b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "!./helloCUDA3"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello thread 0 in block 1\n",
            "Hello thread 1 in block 1\n",
            "Hello thread 2 in block 1\n",
            "Hello thread 3 in block 1\n",
            "Hello thread 0 in block 0\n",
            "Hello thread 1 in block 0\n",
            "Hello thread 2 in block 0\n",
            "Hello thread 3 in block 0\n",
            "Hello thread 0 in block 2\n",
            "Hello thread 1 in block 2\n",
            "Hello thread 2 in block 2\n",
            "Hello thread 3 in block 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hP-QEuUxGeZv",
        "colab_type": "code",
        "outputId": "a46e0612-db70-43bf-9dc3-cdf1e0ffcff6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        }
      },
      "source": [
        "!nvprof --print-gpu-trace ./helloCUDA"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==882== NVPROF is profiling process 882, command: ./helloCUDA\n",
            "Hello thread 0 in block 2\n",
            "Hello thread 1 in block 2\n",
            "Hello thread 2 in block 2\n",
            "Hello thread 3 in block 2\n",
            "Hello thread 0 in block 1\n",
            "Hello thread 1 in block 1\n",
            "Hello thread 2 in block 1\n",
            "Hello thread 3 in block 1\n",
            "Hello thread 0 in block 0\n",
            "Hello thread 1 in block 0\n",
            "Hello thread 2 in block 0\n",
            "Hello thread 3 in block 0\n",
            "==882== Profiling application: ./helloCUDA\n",
            "==882== Profiling result:\n",
            "   Start  Duration            Grid Size      Block Size     Regs*    SSMem*    DSMem*           Device   Context    Stream  Name\n",
            "411.15ms  92.760us              (3 1 1)         (4 1 1)        32        0B        0B     Tesla T4 (0)         1         7  helloCUDA(void) [106]\n",
            "\n",
            "Regs: Number of registers used per CUDA thread. This number includes registers used internally by the CUDA driver and/or tools and can be more than what the compiler shows.\n",
            "SSMem: Static shared memory allocated per CUDA block.\n",
            "DSMem: Dynamic shared memory allocated per CUDA block.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtwajQGsGjTY",
        "colab_type": "code",
        "outputId": "a2804051-647f-4ae7-aae4-a90ee99f8398",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "%%writefile simpleCUDA.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "__global__ void kernel1( int *a )\n",
        "{\n",
        "   int idx = blockIdx.x*blockDim.x + threadIdx.x;\n",
        "   a[idx] = 7;          // output: 7 7 7 7   7 7 7 7   7 7 7 7   7 7 7 7\n",
        "}\n",
        "\n",
        "__global__ void kernel2( int *a )\n",
        "{\n",
        " int idx = blockIdx.x*blockDim.x + threadIdx.x;\n",
        "   a[idx] = blockIdx.x; // output: 0 0 0 0   1 1 1 1   2 2 2 2   3 3 3 3\n",
        "}\n",
        "\n",
        "__global__ void kernel3( int *a )\n",
        "{\n",
        " int idx = blockIdx.x*blockDim.x + threadIdx.x;\n",
        "   a[idx] = threadIdx.x;        // output: 0 1 2 3   1 2 3 4   0 1 2 3   0 1 2 3\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "  int *host_array;\n",
        "  int *dev_array;\n",
        "\n",
        "  host_array = (int *) malloc(sizeof(int)*16);\n",
        "  cudaMalloc(&dev_array, sizeof(int)*16);\n",
        "  cudaMemset(dev_array, 0, 16);\n",
        "    \n",
        "  kernel1<<<4, 4>>>(dev_array);\n",
        "    \n",
        "  cudaMemcpy(host_array, dev_array, sizeof(int)*16, cudaMemcpyDeviceToHost);\n",
        "  \n",
        "  for(int i = 0; i < 16; i++) printf(\" %d \", host_array[i]);\n",
        "  printf(\"\\n\");\n",
        "\n",
        "  cudaMemset(dev_array, 0, 16);\n",
        "    \n",
        "  kernel2<<<4, 4>>>(dev_array);\n",
        "    \n",
        "  cudaMemcpy(host_array, dev_array, sizeof(int)*16, cudaMemcpyDeviceToHost);\n",
        "  \n",
        "  for(int i = 0; i < 16; i++) printf(\" %d \", host_array[i]);\n",
        "  printf(\"\\n\");\n",
        "\n",
        "  cudaMemset(dev_array, 0, 16);\n",
        "    \n",
        "  kernel3<<<4, 4>>>(dev_array);\n",
        "    \n",
        "  cudaMemcpy(host_array, dev_array, sizeof(int)*16, cudaMemcpyDeviceToHost);\n",
        "  \n",
        "  for(int i = 0; i < 16; i++) printf(\" %d \", host_array[i]);\n",
        "  printf(\"\\n\");\n",
        "    \n",
        "  free(host_array);\n",
        "  cudaFree(dev_array);\n",
        "  cudaDeviceReset();\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting simpleCUDA.cu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5lnZlXamknt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvcc -o simpleCUDA simpleCUDA.cu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stfSsKdQmnq3",
        "colab_type": "code",
        "outputId": "3438fe4d-23f9-4e29-b5cd-19526839179c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "!./simpleCUDA"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7 \n",
            " 0  0  0  0  1  1  1  1  2  2  2  2  3  3  3  3 \n",
            " 0  1  2  3  0  1  2  3  0  1  2  3  0  1  2  3 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-WsdbMLmrGd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}